{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import datetime\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_COLUMN_NAMES = ['Age', 'Gender', 'TB', 'DB', 'ALK', 'SGPT', 'SGOT', 'TP', 'ALB', 'AG_Ratio', 'Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"training_data.csv\", names=CSV_COLUMN_NAMES, header=0) \n",
    "test_data = pd.read_csv(\"testing_data.csv\", names=CSV_COLUMN_NAMES, header=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=train_data.replace('?', np.NaN)\n",
    "test_data=test_data.replace('?', np.NaN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_dict = {\n",
    "    'TB' : float,\n",
    "    'DB' : float,\n",
    "    'ALK' : float,\n",
    "    'SGPT' : float,\n",
    "    'SGOT' : float,\n",
    "    'TP' : float,\n",
    "    'ALB' : float,\n",
    "    'AG_Ratio' : float\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.astype(convert_dict) \n",
    "test_data = test_data.astype(convert_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN = train_data.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_new = train_data.fillna(value=MEAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_new[\"Gender\"]=train_data_new[\"Gender\"].replace('Female',1)\n",
    "train_data_new[\"Gender\"]=train_data_new[\"Gender\"].replace('Male',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_new[\"Class\"]=train_data_new[\"Class\"].replace({'Yes':1, 'No':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class0 = train_data_new[\"Class\"] == 0\n",
    "train_data_copy = train_data_new[class0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_new = train_data_new.append(train_data_copy, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_new = train_data_new.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_class = train_data_new.pop(\"Class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEAN_T = test_data.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_new = test_data.fillna(value=MEAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_new[\"Gender\"]=test_data_new[\"Gender\"].replace('Female',1)\n",
    "test_data_new[\"Gender\"]=test_data_new[\"Gender\"].replace('Male',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_class = test_data_new.pop(\"Class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_class=test_class.replace({'Yes':1, 'No':0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(train_data_new.values)\n",
    "\n",
    "params = scaler.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_scaled = scaler.transform(train_data_new.values)\n",
    "train_data_scaled = pd.DataFrame(train_data_scaled, index=train_data_new.index, columns=train_data_new.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>TB</th>\n",
       "      <th>DB</th>\n",
       "      <th>ALK</th>\n",
       "      <th>SGPT</th>\n",
       "      <th>SGOT</th>\n",
       "      <th>TP</th>\n",
       "      <th>ALB</th>\n",
       "      <th>AG_Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7.500000e+02</td>\n",
       "      <td>7.500000e+02</td>\n",
       "      <td>7.500000e+02</td>\n",
       "      <td>7.500000e+02</td>\n",
       "      <td>7.500000e+02</td>\n",
       "      <td>7.500000e+02</td>\n",
       "      <td>7.500000e+02</td>\n",
       "      <td>7.500000e+02</td>\n",
       "      <td>7.500000e+02</td>\n",
       "      <td>7.500000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-1.894781e-17</td>\n",
       "      <td>-9.473903e-18</td>\n",
       "      <td>-6.394885e-17</td>\n",
       "      <td>2.605323e-17</td>\n",
       "      <td>1.575036e-16</td>\n",
       "      <td>2.842171e-17</td>\n",
       "      <td>-6.631732e-17</td>\n",
       "      <td>-1.894781e-17</td>\n",
       "      <td>2.368476e-16</td>\n",
       "      <td>1.942150e-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.000667e+00</td>\n",
       "      <td>1.000667e+00</td>\n",
       "      <td>1.000667e+00</td>\n",
       "      <td>1.000667e+00</td>\n",
       "      <td>1.000667e+00</td>\n",
       "      <td>1.000667e+00</td>\n",
       "      <td>1.000667e+00</td>\n",
       "      <td>1.000667e+00</td>\n",
       "      <td>1.000667e+00</td>\n",
       "      <td>1.000667e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.434528e+00</td>\n",
       "      <td>-5.865885e-01</td>\n",
       "      <td>-4.367014e-01</td>\n",
       "      <td>-4.528309e-01</td>\n",
       "      <td>-9.407155e-01</td>\n",
       "      <td>-3.710287e-01</td>\n",
       "      <td>-3.301868e-01</td>\n",
       "      <td>-3.524273e+00</td>\n",
       "      <td>-2.874246e+00</td>\n",
       "      <td>-2.121615e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-7.288800e-01</td>\n",
       "      <td>-5.865885e-01</td>\n",
       "      <td>-3.647818e-01</td>\n",
       "      <td>-4.132453e-01</td>\n",
       "      <td>-4.579996e-01</td>\n",
       "      <td>-2.971755e-01</td>\n",
       "      <td>-2.756432e-01</td>\n",
       "      <td>-6.483731e-01</td>\n",
       "      <td>-7.351417e-01</td>\n",
       "      <td>-5.731116e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6.302774e-02</td>\n",
       "      <td>-5.865885e-01</td>\n",
       "      <td>-3.468019e-01</td>\n",
       "      <td>-3.736598e-01</td>\n",
       "      <td>-3.295706e-01</td>\n",
       "      <td>-2.356311e-01</td>\n",
       "      <td>-2.172037e-01</td>\n",
       "      <td>9.379457e-02</td>\n",
       "      <td>1.983618e-02</td>\n",
       "      <td>1.133385e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.940195e-01</td>\n",
       "      <td>1.704773e+00</td>\n",
       "      <td>-1.355380e-01</td>\n",
       "      <td>-9.656073e-02</td>\n",
       "      <td>6.014498e-02</td>\n",
       "      <td>-9.407903e-02</td>\n",
       "      <td>-8.084474e-02</td>\n",
       "      <td>6.504203e-01</td>\n",
       "      <td>7.748140e-01</td>\n",
       "      <td>4.326177e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.804247e+00</td>\n",
       "      <td>1.704773e+00</td>\n",
       "      <td>1.297631e+01</td>\n",
       "      <td>7.305942e+00</td>\n",
       "      <td>8.124601e+00</td>\n",
       "      <td>1.187630e+01</td>\n",
       "      <td>1.883409e+01</td>\n",
       "      <td>2.876923e+00</td>\n",
       "      <td>2.913918e+00</td>\n",
       "      <td>5.860363e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Age        Gender            TB            DB           ALK  \\\n",
       "count  7.500000e+02  7.500000e+02  7.500000e+02  7.500000e+02  7.500000e+02   \n",
       "mean  -1.894781e-17 -9.473903e-18 -6.394885e-17  2.605323e-17  1.575036e-16   \n",
       "std    1.000667e+00  1.000667e+00  1.000667e+00  1.000667e+00  1.000667e+00   \n",
       "min   -2.434528e+00 -5.865885e-01 -4.367014e-01 -4.528309e-01 -9.407155e-01   \n",
       "25%   -7.288800e-01 -5.865885e-01 -3.647818e-01 -4.132453e-01 -4.579996e-01   \n",
       "50%    6.302774e-02 -5.865885e-01 -3.468019e-01 -3.736598e-01 -3.295706e-01   \n",
       "75%    7.940195e-01  1.704773e+00 -1.355380e-01 -9.656073e-02  6.014498e-02   \n",
       "max    2.804247e+00  1.704773e+00  1.297631e+01  7.305942e+00  8.124601e+00   \n",
       "\n",
       "               SGPT          SGOT            TP           ALB      AG_Ratio  \n",
       "count  7.500000e+02  7.500000e+02  7.500000e+02  7.500000e+02  7.500000e+02  \n",
       "mean   2.842171e-17 -6.631732e-17 -1.894781e-17  2.368476e-16  1.942150e-16  \n",
       "std    1.000667e+00  1.000667e+00  1.000667e+00  1.000667e+00  1.000667e+00  \n",
       "min   -3.710287e-01 -3.301868e-01 -3.524273e+00 -2.874246e+00 -2.121615e+00  \n",
       "25%   -2.971755e-01 -2.756432e-01 -6.483731e-01 -7.351417e-01 -5.731116e-01  \n",
       "50%   -2.356311e-01 -2.172037e-01  9.379457e-02  1.983618e-02  1.133385e-01  \n",
       "75%   -9.407903e-02 -8.084474e-02  6.504203e-01  7.748140e-01  4.326177e-01  \n",
       "max    1.187630e+01  1.883409e+01  2.876923e+00  2.913918e+00  5.860363e+00  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_scaled = scaler.transform(test_data_new.values)\n",
    "test_data_scaled = pd.DataFrame(test_data_scaled, index=test_data_new.index, columns=test_data_new.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data_scaled.to_numpy()\n",
    "y_train = train_class.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_data_scaled.to_numpy()\n",
    "y_test = test_class.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network \n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, Dropout, Activation\n",
    "\n",
    "do = 0.5\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "model.add(Dense(50, input_dim=10))\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "model.add(Dropout(do))\n",
    "\n",
    "\n",
    "model.add(Dense(50))\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "model.add(Dropout(do))\n",
    "\n",
    "model.add(Dense(50))\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "model.add(Dropout(do))\n",
    "\n",
    "model.add(Dense(20))\n",
    "model.add(LeakyReLU(alpha=0.01))\n",
    "model.add(Dropout(do))\n",
    "\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'temp.h5'\n",
    "\n",
    "\n",
    "keras_callbacks   = [\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1),\n",
    "       ModelCheckpoint(\n",
    "            filepath= path,\n",
    "            save_weights_only=True,\n",
    "            monitor='val_loss',\n",
    "            mode='min',\n",
    "            save_best_only=True,\n",
    "            verbose=1)\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "52/60 [=========================>....] - ETA: 0s - loss: 0.7124 - accuracy: 0.5192\n",
      "Epoch 00001: val_loss improved from inf to 0.67114, saving model to temp.h5\n",
      "60/60 [==============================] - 2s 28ms/step - loss: 0.7078 - accuracy: 0.5150 - val_loss: 0.6711 - val_accuracy: 0.5733\n",
      "Epoch 2/200\n",
      "51/60 [========================>.....] - ETA: 0s - loss: 0.6750 - accuracy: 0.5294\n",
      "Epoch 00002: val_loss improved from 0.67114 to 0.65879, saving model to temp.h5\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.6705 - accuracy: 0.5317 - val_loss: 0.6588 - val_accuracy: 0.5667\n",
      "Epoch 3/200\n",
      "51/60 [========================>.....] - ETA: 0s - loss: 0.6798 - accuracy: 0.5490\n",
      "Epoch 00003: val_loss improved from 0.65879 to 0.64633, saving model to temp.h5\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.6696 - accuracy: 0.5633 - val_loss: 0.6463 - val_accuracy: 0.6267\n",
      "Epoch 4/200\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.6618 - accuracy: 0.5678\n",
      "Epoch 00004: val_loss improved from 0.64633 to 0.63605, saving model to temp.h5\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.6597 - accuracy: 0.5733 - val_loss: 0.6361 - val_accuracy: 0.6000\n",
      "Epoch 5/200\n",
      "52/60 [=========================>....] - ETA: 0s - loss: 0.6478 - accuracy: 0.5596\n",
      "Epoch 00005: val_loss improved from 0.63605 to 0.62407, saving model to temp.h5\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.6477 - accuracy: 0.5533 - val_loss: 0.6241 - val_accuracy: 0.6200\n",
      "Epoch 6/200\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 0.6425 - accuracy: 0.5946\n",
      "Epoch 00006: val_loss improved from 0.62407 to 0.61970, saving model to temp.h5\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.6401 - accuracy: 0.5983 - val_loss: 0.6197 - val_accuracy: 0.6267\n",
      "Epoch 7/200\n",
      "50/60 [========================>.....] - ETA: 0s - loss: 0.6214 - accuracy: 0.6200\n",
      "Epoch 00007: val_loss improved from 0.61970 to 0.61809, saving model to temp.h5\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.6326 - accuracy: 0.6167 - val_loss: 0.6181 - val_accuracy: 0.6200\n",
      "Epoch 8/200\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 0.6149 - accuracy: 0.6339\n",
      "Epoch 00008: val_loss improved from 0.61809 to 0.61253, saving model to temp.h5\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.6122 - accuracy: 0.6383 - val_loss: 0.6125 - val_accuracy: 0.6267\n",
      "Epoch 9/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.6157 - accuracy: 0.6167\n",
      "Epoch 00009: val_loss improved from 0.61253 to 0.59235, saving model to temp.h5\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.6157 - accuracy: 0.6167 - val_loss: 0.5923 - val_accuracy: 0.6467\n",
      "Epoch 10/200\n",
      "54/60 [==========================>...] - ETA: 0s - loss: 0.5986 - accuracy: 0.6241\n",
      "Epoch 00010: val_loss improved from 0.59235 to 0.58941, saving model to temp.h5\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.6088 - accuracy: 0.6233 - val_loss: 0.5894 - val_accuracy: 0.6400\n",
      "Epoch 11/200\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.6097 - accuracy: 0.6552\n",
      "Epoch 00011: val_loss did not improve from 0.58941\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.6114 - accuracy: 0.6533 - val_loss: 0.5907 - val_accuracy: 0.6400\n",
      "Epoch 12/200\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 0.6103 - accuracy: 0.6446\n",
      "Epoch 00012: val_loss did not improve from 0.58941\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.6161 - accuracy: 0.6417 - val_loss: 0.5895 - val_accuracy: 0.6333\n",
      "Epoch 13/200\n",
      "53/60 [=========================>....] - ETA: 0s - loss: 0.6022 - accuracy: 0.6453\n",
      "Epoch 00013: val_loss improved from 0.58941 to 0.58744, saving model to temp.h5\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.6016 - accuracy: 0.6517 - val_loss: 0.5874 - val_accuracy: 0.6467\n",
      "Epoch 14/200\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 0.5874 - accuracy: 0.6625\n",
      "Epoch 00014: val_loss improved from 0.58744 to 0.57939, saving model to temp.h5\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.5944 - accuracy: 0.6667 - val_loss: 0.5794 - val_accuracy: 0.6333\n",
      "Epoch 15/200\n",
      "47/60 [======================>.......] - ETA: 0s - loss: 0.5875 - accuracy: 0.6489\n",
      "Epoch 00015: val_loss improved from 0.57939 to 0.57578, saving model to temp.h5\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.5911 - accuracy: 0.6567 - val_loss: 0.5758 - val_accuracy: 0.6333\n",
      "Epoch 16/200\n",
      "52/60 [=========================>....] - ETA: 0s - loss: 0.6188 - accuracy: 0.6481\n",
      "Epoch 00016: val_loss did not improve from 0.57578\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.6078 - accuracy: 0.6500 - val_loss: 0.5801 - val_accuracy: 0.6333\n",
      "Epoch 17/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5831 - accuracy: 0.6717\n",
      "Epoch 00017: val_loss improved from 0.57578 to 0.57472, saving model to temp.h5\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.5831 - accuracy: 0.6717 - val_loss: 0.5747 - val_accuracy: 0.6733\n",
      "Epoch 18/200\n",
      "50/60 [========================>.....] - ETA: 0s - loss: 0.6095 - accuracy: 0.6720\n",
      "Epoch 00018: val_loss did not improve from 0.57472\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.6138 - accuracy: 0.6633 - val_loss: 0.5827 - val_accuracy: 0.6600\n",
      "Epoch 19/200\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.5852 - accuracy: 0.6684\n",
      "Epoch 00019: val_loss did not improve from 0.57472\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.5940 - accuracy: 0.6667 - val_loss: 0.5794 - val_accuracy: 0.6600\n",
      "Epoch 20/200\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.5840 - accuracy: 0.6627\n",
      "Epoch 00020: val_loss did not improve from 0.57472\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.5828 - accuracy: 0.6650 - val_loss: 0.5773 - val_accuracy: 0.6667\n",
      "Epoch 21/200\n",
      "53/60 [=========================>....] - ETA: 0s - loss: 0.5990 - accuracy: 0.6830\n",
      "Epoch 00021: val_loss did not improve from 0.57472\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 0.5926 - accuracy: 0.6783 - val_loss: 0.5757 - val_accuracy: 0.6733\n",
      "Epoch 22/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5739 - accuracy: 0.6833\n",
      "Epoch 00022: val_loss improved from 0.57472 to 0.57025, saving model to temp.h5\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.5739 - accuracy: 0.6833 - val_loss: 0.5703 - val_accuracy: 0.6867\n",
      "Epoch 23/200\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.5785 - accuracy: 0.6672\n",
      "Epoch 00023: val_loss improved from 0.57025 to 0.56821, saving model to temp.h5\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.5805 - accuracy: 0.6667 - val_loss: 0.5682 - val_accuracy: 0.6733\n",
      "Epoch 24/200\n",
      "51/60 [========================>.....] - ETA: 0s - loss: 0.5586 - accuracy: 0.7255\n",
      "Epoch 00024: val_loss improved from 0.56821 to 0.56536, saving model to temp.h5\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.5749 - accuracy: 0.7033 - val_loss: 0.5654 - val_accuracy: 0.6800\n",
      "Epoch 25/200\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.5572 - accuracy: 0.6807\n",
      "Epoch 00025: val_loss did not improve from 0.56536\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.5651 - accuracy: 0.6717 - val_loss: 0.5674 - val_accuracy: 0.6800\n",
      "Epoch 26/200\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.5717 - accuracy: 0.6947\n",
      "Epoch 00026: val_loss improved from 0.56536 to 0.56035, saving model to temp.h5\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5758 - accuracy: 0.6933 - val_loss: 0.5603 - val_accuracy: 0.6933\n",
      "Epoch 27/200\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 0.5763 - accuracy: 0.6732\n",
      "Epoch 00027: val_loss did not improve from 0.56035\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5781 - accuracy: 0.6733 - val_loss: 0.5642 - val_accuracy: 0.6800\n",
      "Epoch 28/200\n",
      "52/60 [=========================>....] - ETA: 0s - loss: 0.5984 - accuracy: 0.6942\n",
      "Epoch 00028: val_loss did not improve from 0.56035\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.5941 - accuracy: 0.6867 - val_loss: 0.5676 - val_accuracy: 0.6733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/200\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.5727 - accuracy: 0.6702\n",
      "Epoch 00029: val_loss did not improve from 0.56035\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5697 - accuracy: 0.6700 - val_loss: 0.5676 - val_accuracy: 0.6733\n",
      "Epoch 30/200\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.5590 - accuracy: 0.6897\n",
      "Epoch 00030: val_loss did not improve from 0.56035\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.5573 - accuracy: 0.6917 - val_loss: 0.5662 - val_accuracy: 0.6800\n",
      "Epoch 31/200\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 0.5720 - accuracy: 0.6893\n",
      "Epoch 00031: val_loss did not improve from 0.56035\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.5730 - accuracy: 0.6883 - val_loss: 0.5649 - val_accuracy: 0.6800\n",
      "Epoch 32/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5669 - accuracy: 0.6833\n",
      "Epoch 00032: val_loss did not improve from 0.56035\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.5669 - accuracy: 0.6833 - val_loss: 0.5659 - val_accuracy: 0.6800\n",
      "Epoch 33/200\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.5604 - accuracy: 0.6831\n",
      "Epoch 00033: val_loss did not improve from 0.56035\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.5604 - accuracy: 0.6850 - val_loss: 0.5614 - val_accuracy: 0.6800\n",
      "Epoch 34/200\n",
      "52/60 [=========================>....] - ETA: 0s - loss: 0.5500 - accuracy: 0.6885\n",
      "Epoch 00034: val_loss did not improve from 0.56035\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.5500 - accuracy: 0.6917 - val_loss: 0.5615 - val_accuracy: 0.6800\n",
      "Epoch 35/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5748 - accuracy: 0.7017\n",
      "Epoch 00035: val_loss did not improve from 0.56035\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.5748 - accuracy: 0.7017 - val_loss: 0.5645 - val_accuracy: 0.6800\n",
      "Epoch 36/200\n",
      "54/60 [==========================>...] - ETA: 0s - loss: 0.5745 - accuracy: 0.6944 ETA: 0s - loss: 0.5775 - accuracy\n",
      "Epoch 00036: val_loss improved from 0.56035 to 0.56030, saving model to temp.h5\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.5705 - accuracy: 0.7050 - val_loss: 0.5603 - val_accuracy: 0.6867\n",
      "Epoch 37/200\n",
      "55/60 [==========================>...] - ETA: 0s - loss: 0.5650 - accuracy: 0.6982\n",
      "Epoch 00037: val_loss did not improve from 0.56030\n",
      "60/60 [==============================] - 1s 8ms/step - loss: 0.5590 - accuracy: 0.7033 - val_loss: 0.5623 - val_accuracy: 0.6733\n",
      "Epoch 38/200\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.5558 - accuracy: 0.6931\n",
      "Epoch 00038: val_loss improved from 0.56030 to 0.55732, saving model to temp.h5\n",
      "60/60 [==============================] - 1s 8ms/step - loss: 0.5574 - accuracy: 0.6867 - val_loss: 0.5573 - val_accuracy: 0.6800\n",
      "Epoch 39/200\n",
      "55/60 [==========================>...] - ETA: 0s - loss: 0.5761 - accuracy: 0.6855\n",
      "Epoch 00039: val_loss did not improve from 0.55732\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.5703 - accuracy: 0.6900 - val_loss: 0.5613 - val_accuracy: 0.6800\n",
      "Epoch 40/200\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.5763 - accuracy: 0.6881\n",
      "Epoch 00040: val_loss did not improve from 0.55732\n",
      "60/60 [==============================] - 1s 8ms/step - loss: 0.5739 - accuracy: 0.6900 - val_loss: 0.5610 - val_accuracy: 0.6800\n",
      "Epoch 41/200\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.5675 - accuracy: 0.6845\n",
      "Epoch 00041: val_loss did not improve from 0.55732\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5614 - accuracy: 0.6883 - val_loss: 0.5617 - val_accuracy: 0.6867\n",
      "Epoch 42/200\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.5438 - accuracy: 0.7053\n",
      "Epoch 00042: val_loss did not improve from 0.55732\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.5463 - accuracy: 0.7050 - val_loss: 0.5576 - val_accuracy: 0.6800\n",
      "Epoch 43/200\n",
      "52/60 [=========================>....] - ETA: 0s - loss: 0.5646 - accuracy: 0.6962\n",
      "Epoch 00043: val_loss improved from 0.55732 to 0.55193, saving model to temp.h5\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.5568 - accuracy: 0.7117 - val_loss: 0.5519 - val_accuracy: 0.6867\n",
      "Epoch 44/200\n",
      "54/60 [==========================>...] - ETA: 0s - loss: 0.5811 - accuracy: 0.6815\n",
      "Epoch 00044: val_loss did not improve from 0.55193\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 0.5772 - accuracy: 0.6867 - val_loss: 0.5592 - val_accuracy: 0.6867\n",
      "Epoch 45/200\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.5478 - accuracy: 0.6797\n",
      "Epoch 00045: val_loss did not improve from 0.55193\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 0.5476 - accuracy: 0.6783 - val_loss: 0.5555 - val_accuracy: 0.6800\n",
      "Epoch 46/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5366 - accuracy: 0.6933\n",
      "Epoch 00046: val_loss did not improve from 0.55193\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 0.5366 - accuracy: 0.6933 - val_loss: 0.5550 - val_accuracy: 0.6733\n",
      "Epoch 47/200\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.5648 - accuracy: 0.7105\n",
      "Epoch 00047: val_loss did not improve from 0.55193\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 0.5640 - accuracy: 0.7100 - val_loss: 0.5536 - val_accuracy: 0.6733\n",
      "Epoch 48/200\n",
      "55/60 [==========================>...] - ETA: 0s - loss: 0.5593 - accuracy: 0.6909\n",
      "Epoch 00048: val_loss did not improve from 0.55193\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 0.5645 - accuracy: 0.6867 - val_loss: 0.5621 - val_accuracy: 0.6600\n",
      "Epoch 49/200\n",
      "47/60 [======================>.......] - ETA: 0s - loss: 0.5788 - accuracy: 0.6872\n",
      "Epoch 00049: val_loss did not improve from 0.55193\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 0.5642 - accuracy: 0.6983 - val_loss: 0.5647 - val_accuracy: 0.6600\n",
      "Epoch 50/200\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.5471 - accuracy: 0.6983\n",
      "Epoch 00050: val_loss did not improve from 0.55193\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.5450 - accuracy: 0.6983 - val_loss: 0.5562 - val_accuracy: 0.6667\n",
      "Epoch 51/200\n",
      "54/60 [==========================>...] - ETA: 0s - loss: 0.5620 - accuracy: 0.6963\n",
      "Epoch 00051: val_loss did not improve from 0.55193\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.5608 - accuracy: 0.6900 - val_loss: 0.5648 - val_accuracy: 0.6600\n",
      "Epoch 52/200\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.5386 - accuracy: 0.7158\n",
      "Epoch 00052: val_loss did not improve from 0.55193\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.5431 - accuracy: 0.7083 - val_loss: 0.5540 - val_accuracy: 0.6933\n",
      "Epoch 53/200\n",
      "54/60 [==========================>...] - ETA: 0s - loss: 0.5500 - accuracy: 0.7241\n",
      "Epoch 00053: val_loss did not improve from 0.55193\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.5536 - accuracy: 0.7183 - val_loss: 0.5530 - val_accuracy: 0.6800\n",
      "Epoch 54/200\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.5627 - accuracy: 0.7103\n",
      "Epoch 00054: val_loss did not improve from 0.55193\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.5597 - accuracy: 0.7133 - val_loss: 0.5549 - val_accuracy: 0.6867\n",
      "Epoch 55/200\n",
      "54/60 [==========================>...] - ETA: 0s - loss: 0.5380 - accuracy: 0.7278\n",
      "Epoch 00055: val_loss improved from 0.55193 to 0.55065, saving model to temp.h5\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5368 - accuracy: 0.7283 - val_loss: 0.5507 - val_accuracy: 0.6800\n",
      "Epoch 56/200\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.5382 - accuracy: 0.6947\n",
      "Epoch 00056: val_loss improved from 0.55065 to 0.54937, saving model to temp.h5\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5526 - accuracy: 0.6883 - val_loss: 0.5494 - val_accuracy: 0.6600\n",
      "Epoch 57/200\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 0.5488 - accuracy: 0.7268\n",
      "Epoch 00057: val_loss improved from 0.54937 to 0.54740, saving model to temp.h5\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5439 - accuracy: 0.7250 - val_loss: 0.5474 - val_accuracy: 0.6800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/200\n",
      "54/60 [==========================>...] - ETA: 0s - loss: 0.5572 - accuracy: 0.7111\n",
      "Epoch 00058: val_loss improved from 0.54740 to 0.54601, saving model to temp.h5\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.5514 - accuracy: 0.7150 - val_loss: 0.5460 - val_accuracy: 0.6867\n",
      "Epoch 59/200\n",
      "54/60 [==========================>...] - ETA: 0s - loss: 0.5399 - accuracy: 0.6981\n",
      "Epoch 00059: val_loss did not improve from 0.54601\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.5515 - accuracy: 0.6967 - val_loss: 0.5542 - val_accuracy: 0.6667\n",
      "Epoch 60/200\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.5548 - accuracy: 0.7085\n",
      "Epoch 00060: val_loss did not improve from 0.54601\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.5535 - accuracy: 0.7100 - val_loss: 0.5524 - val_accuracy: 0.6800\n",
      "Epoch 61/200\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.5287 - accuracy: 0.6982\n",
      "Epoch 00061: val_loss did not improve from 0.54601\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.5288 - accuracy: 0.7017 - val_loss: 0.5472 - val_accuracy: 0.6800\n",
      "Epoch 62/200\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 0.5543 - accuracy: 0.7143\n",
      "Epoch 00062: val_loss did not improve from 0.54601\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.5446 - accuracy: 0.7233 - val_loss: 0.5461 - val_accuracy: 0.6733\n",
      "Epoch 63/200\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.5640 - accuracy: 0.7035\n",
      "Epoch 00063: val_loss did not improve from 0.54601\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.5620 - accuracy: 0.7100 - val_loss: 0.5482 - val_accuracy: 0.6667\n",
      "Epoch 64/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5429 - accuracy: 0.7167\n",
      "Epoch 00064: val_loss improved from 0.54601 to 0.54234, saving model to temp.h5\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.5429 - accuracy: 0.7167 - val_loss: 0.5423 - val_accuracy: 0.6800\n",
      "Epoch 65/200\n",
      "52/60 [=========================>....] - ETA: 0s - loss: 0.5364 - accuracy: 0.7000\n",
      "Epoch 00065: val_loss did not improve from 0.54234\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.5334 - accuracy: 0.7083 - val_loss: 0.5448 - val_accuracy: 0.6667\n",
      "Epoch 66/200\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.5394 - accuracy: 0.7000\n",
      "Epoch 00066: val_loss did not improve from 0.54234\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.5367 - accuracy: 0.7050 - val_loss: 0.5491 - val_accuracy: 0.6733\n",
      "Epoch 67/200\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.5368 - accuracy: 0.6982\n",
      "Epoch 00067: val_loss did not improve from 0.54234\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.5318 - accuracy: 0.7017 - val_loss: 0.5484 - val_accuracy: 0.6800\n",
      "Epoch 68/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5291 - accuracy: 0.7167\n",
      "Epoch 00068: val_loss did not improve from 0.54234\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.5291 - accuracy: 0.7167 - val_loss: 0.5517 - val_accuracy: 0.6533\n",
      "Epoch 69/200\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.5218 - accuracy: 0.7158\n",
      "Epoch 00069: val_loss did not improve from 0.54234\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5198 - accuracy: 0.7200 - val_loss: 0.5458 - val_accuracy: 0.6733\n",
      "Epoch 70/200\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.5277 - accuracy: 0.7211\n",
      "Epoch 00070: val_loss did not improve from 0.54234\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.5311 - accuracy: 0.7200 - val_loss: 0.5460 - val_accuracy: 0.6800\n",
      "Epoch 71/200\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 0.5379 - accuracy: 0.7054\n",
      "Epoch 00071: val_loss did not improve from 0.54234\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.5373 - accuracy: 0.7100 - val_loss: 0.5433 - val_accuracy: 0.6667\n",
      "Epoch 72/200\n",
      "55/60 [==========================>...] - ETA: 0s - loss: 0.5227 - accuracy: 0.7091\n",
      "Epoch 00072: val_loss improved from 0.54234 to 0.53965, saving model to temp.h5\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.5221 - accuracy: 0.7150 - val_loss: 0.5396 - val_accuracy: 0.6667\n",
      "Epoch 73/200\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.5358 - accuracy: 0.7068\n",
      "Epoch 00073: val_loss did not improve from 0.53965\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.5343 - accuracy: 0.7083 - val_loss: 0.5435 - val_accuracy: 0.6733\n",
      "Epoch 74/200\n",
      "55/60 [==========================>...] - ETA: 0s - loss: 0.5395 - accuracy: 0.7055\n",
      "Epoch 00074: val_loss did not improve from 0.53965\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.5355 - accuracy: 0.7100 - val_loss: 0.5442 - val_accuracy: 0.6800\n",
      "Epoch 75/200\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.5201 - accuracy: 0.7207\n",
      "Epoch 00075: val_loss did not improve from 0.53965\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5208 - accuracy: 0.7233 - val_loss: 0.5462 - val_accuracy: 0.6667\n",
      "Epoch 76/200\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.5155 - accuracy: 0.7175\n",
      "Epoch 00076: val_loss improved from 0.53965 to 0.53879, saving model to temp.h5\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.5120 - accuracy: 0.7183 - val_loss: 0.5388 - val_accuracy: 0.6667\n",
      "Epoch 77/200\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 0.5181 - accuracy: 0.7143\n",
      "Epoch 00077: val_loss did not improve from 0.53879\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.5160 - accuracy: 0.7167 - val_loss: 0.5413 - val_accuracy: 0.6733\n",
      "Epoch 78/200\n",
      "48/60 [=======================>......] - ETA: 0s - loss: 0.5180 - accuracy: 0.7271\n",
      "Epoch 00078: val_loss did not improve from 0.53879\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 0.5233 - accuracy: 0.7150 - val_loss: 0.5458 - val_accuracy: 0.6733\n",
      "Epoch 79/200\n",
      "55/60 [==========================>...] - ETA: 0s - loss: 0.5382 - accuracy: 0.7164\n",
      "Epoch 00079: val_loss did not improve from 0.53879\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.5340 - accuracy: 0.7200 - val_loss: 0.5558 - val_accuracy: 0.6467\n",
      "Epoch 80/200\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 0.5398 - accuracy: 0.7179\n",
      "Epoch 00080: val_loss did not improve from 0.53879\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.5341 - accuracy: 0.7233 - val_loss: 0.5543 - val_accuracy: 0.6533\n",
      "Epoch 81/200\n",
      "51/60 [========================>.....] - ETA: 0s - loss: 0.5549 - accuracy: 0.7078\n",
      "Epoch 00081: val_loss did not improve from 0.53879\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.5413 - accuracy: 0.7100 - val_loss: 0.5521 - val_accuracy: 0.6467\n",
      "Epoch 82/200\n",
      "54/60 [==========================>...] - ETA: 0s - loss: 0.5589 - accuracy: 0.7130\n",
      "Epoch 00082: val_loss did not improve from 0.53879\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.5585 - accuracy: 0.7017 - val_loss: 0.5454 - val_accuracy: 0.6400\n",
      "Epoch 83/200\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.5248 - accuracy: 0.7220\n",
      "Epoch 00083: val_loss did not improve from 0.53879\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 0.5239 - accuracy: 0.7217 - val_loss: 0.5448 - val_accuracy: 0.6467\n",
      "Epoch 84/200\n",
      "45/60 [=====================>........] - ETA: 0s - loss: 0.5294 - accuracy: 0.7067\n",
      "Epoch 00084: val_loss did not improve from 0.53879\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.5211 - accuracy: 0.7150 - val_loss: 0.5421 - val_accuracy: 0.6533\n",
      "Epoch 85/200\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.5264 - accuracy: 0.7069\n",
      "Epoch 00085: val_loss did not improve from 0.53879\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.5278 - accuracy: 0.7050 - val_loss: 0.5429 - val_accuracy: 0.6600\n",
      "Epoch 86/200\n",
      "53/60 [=========================>....] - ETA: 0s - loss: 0.5307 - accuracy: 0.7245\n",
      "Epoch 00086: val_loss did not improve from 0.53879\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.5245 - accuracy: 0.7333 - val_loss: 0.5454 - val_accuracy: 0.6667\n",
      "Epoch 87/200\n",
      "51/60 [========================>.....] - ETA: 0s - loss: 0.5379 - accuracy: 0.7137\n",
      "Epoch 00087: val_loss did not improve from 0.53879\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.5312 - accuracy: 0.7167 - val_loss: 0.5466 - val_accuracy: 0.6533\n",
      "Epoch 88/200\n",
      "50/60 [========================>.....] - ETA: 0s - loss: 0.5124 - accuracy: 0.7240\n",
      "Epoch 00088: val_loss did not improve from 0.53879\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.5139 - accuracy: 0.7283 - val_loss: 0.5388 - val_accuracy: 0.6533\n",
      "Epoch 89/200\n",
      "55/60 [==========================>...] - ETA: 0s - loss: 0.5186 - accuracy: 0.7145\n",
      "Epoch 00089: val_loss improved from 0.53879 to 0.53841, saving model to temp.h5\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.5216 - accuracy: 0.7183 - val_loss: 0.5384 - val_accuracy: 0.6533\n",
      "Epoch 90/200\n",
      "54/60 [==========================>...] - ETA: 0s - loss: 0.5083 - accuracy: 0.7204\n",
      "Epoch 00090: val_loss did not improve from 0.53841\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.5062 - accuracy: 0.7217 - val_loss: 0.5418 - val_accuracy: 0.6600\n",
      "Epoch 91/200\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.5192 - accuracy: 0.7140\n",
      "Epoch 00091: val_loss did not improve from 0.53841\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.5193 - accuracy: 0.7150 - val_loss: 0.5440 - val_accuracy: 0.6600\n",
      "Epoch 92/200\n",
      "54/60 [==========================>...] - ETA: 0s - loss: 0.5040 - accuracy: 0.7481\n",
      "Epoch 00092: val_loss improved from 0.53841 to 0.53742, saving model to temp.h5\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.5123 - accuracy: 0.7383 - val_loss: 0.5374 - val_accuracy: 0.6600\n",
      "Epoch 93/200\n",
      "53/60 [=========================>....] - ETA: 0s - loss: 0.5089 - accuracy: 0.7264\n",
      "Epoch 00093: val_loss did not improve from 0.53742\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.5053 - accuracy: 0.7267 - val_loss: 0.5424 - val_accuracy: 0.6667\n",
      "Epoch 94/200\n",
      "49/60 [=======================>......] - ETA: 0s - loss: 0.5076 - accuracy: 0.7327\n",
      "Epoch 00094: val_loss did not improve from 0.53742\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 0.5005 - accuracy: 0.7450 - val_loss: 0.5426 - val_accuracy: 0.6733\n",
      "Epoch 95/200\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.5103 - accuracy: 0.7368\n",
      "Epoch 00095: val_loss did not improve from 0.53742\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5097 - accuracy: 0.7350 - val_loss: 0.5403 - val_accuracy: 0.6800\n",
      "Epoch 96/200\n",
      "55/60 [==========================>...] - ETA: 0s - loss: 0.5349 - accuracy: 0.7091\n",
      "Epoch 00096: val_loss did not improve from 0.53742\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5290 - accuracy: 0.7150 - val_loss: 0.5391 - val_accuracy: 0.6733\n",
      "Epoch 97/200\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.5026 - accuracy: 0.7310\n",
      "Epoch 00097: val_loss improved from 0.53742 to 0.53301, saving model to temp.h5\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5051 - accuracy: 0.7317 - val_loss: 0.5330 - val_accuracy: 0.6800\n",
      "Epoch 98/200\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.5059 - accuracy: 0.7241 ETA: 0s - loss: 0.5132 - accura\n",
      "Epoch 00098: val_loss improved from 0.53301 to 0.52587, saving model to temp.h5\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.5021 - accuracy: 0.7233 - val_loss: 0.5259 - val_accuracy: 0.6733\n",
      "Epoch 99/200\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.5309 - accuracy: 0.7123\n",
      "Epoch 00099: val_loss did not improve from 0.52587\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.5280 - accuracy: 0.7167 - val_loss: 0.5290 - val_accuracy: 0.6800\n",
      "Epoch 100/200\n",
      "55/60 [==========================>...] - ETA: 0s - loss: 0.5003 - accuracy: 0.7382\n",
      "Epoch 00100: val_loss did not improve from 0.52587\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.5141 - accuracy: 0.7300 - val_loss: 0.5261 - val_accuracy: 0.6800\n",
      "Epoch 101/200\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.5200 - accuracy: 0.7000\n",
      "Epoch 00101: val_loss did not improve from 0.52587\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.5217 - accuracy: 0.7000 - val_loss: 0.5339 - val_accuracy: 0.6600\n",
      "Epoch 102/200\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 0.5036 - accuracy: 0.7250\n",
      "Epoch 00102: val_loss did not improve from 0.52587\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.5042 - accuracy: 0.7283 - val_loss: 0.5341 - val_accuracy: 0.6733\n",
      "Epoch 103/200\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 0.5230 - accuracy: 0.7036\n",
      "Epoch 00103: val_loss did not improve from 0.52587\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.5202 - accuracy: 0.7133 - val_loss: 0.5330 - val_accuracy: 0.6733\n",
      "Epoch 104/200\n",
      "54/60 [==========================>...] - ETA: 0s - loss: 0.5243 - accuracy: 0.7241\n",
      "Epoch 00104: val_loss did not improve from 0.52587\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.5167 - accuracy: 0.7300 - val_loss: 0.5314 - val_accuracy: 0.6667\n",
      "Epoch 105/200\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.5050 - accuracy: 0.7207\n",
      "Epoch 00105: val_loss did not improve from 0.52587\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.5007 - accuracy: 0.7250 - val_loss: 0.5293 - val_accuracy: 0.6667\n",
      "Epoch 106/200\n",
      "45/60 [=====================>........] - ETA: 0s - loss: 0.5140 - accuracy: 0.7267\n",
      "Epoch 00106: val_loss did not improve from 0.52587\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 0.5150 - accuracy: 0.7183 - val_loss: 0.5320 - val_accuracy: 0.6533\n",
      "Epoch 107/200\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.5057 - accuracy: 0.7288\n",
      "Epoch 00107: val_loss did not improve from 0.52587\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.5039 - accuracy: 0.7317 - val_loss: 0.5310 - val_accuracy: 0.6667\n",
      "Epoch 108/200\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 0.5140 - accuracy: 0.7161\n",
      "Epoch 00108: val_loss did not improve from 0.52587\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 0.5120 - accuracy: 0.7183 - val_loss: 0.5321 - val_accuracy: 0.6867\n",
      "Epoch 109/200\n",
      "53/60 [=========================>....] - ETA: 0s - loss: 0.4987 - accuracy: 0.7245\n",
      "Epoch 00109: val_loss did not improve from 0.52587\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.4937 - accuracy: 0.7267 - val_loss: 0.5261 - val_accuracy: 0.6733\n",
      "Epoch 110/200\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 0.5060 - accuracy: 0.7411\n",
      "Epoch 00110: val_loss did not improve from 0.52587\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.5085 - accuracy: 0.7417 - val_loss: 0.5350 - val_accuracy: 0.6600\n",
      "Epoch 111/200\n",
      "55/60 [==========================>...] - ETA: 0s - loss: 0.5060 - accuracy: 0.7473\n",
      "Epoch 00111: val_loss did not improve from 0.52587\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.5077 - accuracy: 0.7400 - val_loss: 0.5387 - val_accuracy: 0.6733\n",
      "Epoch 112/200\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 0.5102 - accuracy: 0.7268\n",
      "Epoch 00112: val_loss did not improve from 0.52587\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 0.5048 - accuracy: 0.7300 - val_loss: 0.5358 - val_accuracy: 0.6733\n",
      "Epoch 113/200\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.5221 - accuracy: 0.7224\n",
      "Epoch 00113: val_loss did not improve from 0.52587\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 0.5193 - accuracy: 0.7233 - val_loss: 0.5372 - val_accuracy: 0.6800\n",
      "Epoch 114/200\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 0.5136 - accuracy: 0.7143\n",
      "Epoch 00114: val_loss did not improve from 0.52587\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 0.5102 - accuracy: 0.7183 - val_loss: 0.5297 - val_accuracy: 0.6733\n",
      "Epoch 115/200\n",
      "53/60 [=========================>....] - ETA: 0s - loss: 0.5079 - accuracy: 0.7453\n",
      "Epoch 00115: val_loss did not improve from 0.52587\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.5097 - accuracy: 0.7450 - val_loss: 0.5280 - val_accuracy: 0.6667\n",
      "Epoch 116/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59/60 [============================>.] - ETA: 0s - loss: 0.4993 - accuracy: 0.7203\n",
      "Epoch 00116: val_loss did not improve from 0.52587\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.4990 - accuracy: 0.7200 - val_loss: 0.5296 - val_accuracy: 0.6733\n",
      "Epoch 117/200\n",
      "51/60 [========================>.....] - ETA: 0s - loss: 0.4963 - accuracy: 0.7275\n",
      "Epoch 00117: val_loss did not improve from 0.52587\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.4904 - accuracy: 0.7350 - val_loss: 0.5446 - val_accuracy: 0.6800\n",
      "Epoch 118/200\n",
      "52/60 [=========================>....] - ETA: 0s - loss: 0.4850 - accuracy: 0.7269\n",
      "Epoch 00118: val_loss did not improve from 0.52587\n",
      "60/60 [==============================] - 1s 8ms/step - loss: 0.4779 - accuracy: 0.7300 - val_loss: 0.5396 - val_accuracy: 0.6667\n",
      "Epoch 119/200\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.5190 - accuracy: 0.7207\n",
      "Epoch 00119: val_loss did not improve from 0.52587\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.5190 - accuracy: 0.7217 - val_loss: 0.5351 - val_accuracy: 0.6867\n",
      "Epoch 120/200\n",
      "53/60 [=========================>....] - ETA: 0s - loss: 0.4990 - accuracy: 0.7453\n",
      "Epoch 00120: val_loss did not improve from 0.52587\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.5057 - accuracy: 0.7333 - val_loss: 0.5312 - val_accuracy: 0.6733\n",
      "Epoch 121/200\n",
      "49/60 [=======================>......] - ETA: 0s - loss: 0.4945 - accuracy: 0.7224\n",
      "Epoch 00121: val_loss improved from 0.52587 to 0.51288, saving model to temp.h5\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.5010 - accuracy: 0.7300 - val_loss: 0.5129 - val_accuracy: 0.6800\n",
      "Epoch 122/200\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.4833 - accuracy: 0.7421\n",
      "Epoch 00122: val_loss improved from 0.51288 to 0.51149, saving model to temp.h5\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.4860 - accuracy: 0.7367 - val_loss: 0.5115 - val_accuracy: 0.6800\n",
      "Epoch 123/200\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.5035 - accuracy: 0.7298\n",
      "Epoch 00123: val_loss did not improve from 0.51149\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 0.5022 - accuracy: 0.7317 - val_loss: 0.5188 - val_accuracy: 0.6933\n",
      "Epoch 124/200\n",
      "55/60 [==========================>...] - ETA: 0s - loss: 0.4910 - accuracy: 0.7418\n",
      "Epoch 00124: val_loss did not improve from 0.51149\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.4901 - accuracy: 0.7433 - val_loss: 0.5136 - val_accuracy: 0.6933\n",
      "Epoch 125/200\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.5043 - accuracy: 0.7458\n",
      "Epoch 00125: val_loss did not improve from 0.51149\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.5051 - accuracy: 0.7450 - val_loss: 0.5155 - val_accuracy: 0.6933\n",
      "Epoch 126/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.5125 - accuracy: 0.7200\n",
      "Epoch 00126: val_loss did not improve from 0.51149\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.5125 - accuracy: 0.7200 - val_loss: 0.5242 - val_accuracy: 0.6800\n",
      "Epoch 127/200\n",
      "48/60 [=======================>......] - ETA: 0s - loss: 0.4952 - accuracy: 0.7333\n",
      "Epoch 00127: val_loss did not improve from 0.51149\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.4996 - accuracy: 0.7217 - val_loss: 0.5174 - val_accuracy: 0.6867\n",
      "Epoch 128/200\n",
      "53/60 [=========================>....] - ETA: 0s - loss: 0.4866 - accuracy: 0.7509\n",
      "Epoch 00128: val_loss did not improve from 0.51149\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.4928 - accuracy: 0.7450 - val_loss: 0.5246 - val_accuracy: 0.6667\n",
      "Epoch 129/200\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.4845 - accuracy: 0.7414\n",
      "Epoch 00129: val_loss did not improve from 0.51149\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.4899 - accuracy: 0.7350 - val_loss: 0.5135 - val_accuracy: 0.6933\n",
      "Epoch 130/200\n",
      "55/60 [==========================>...] - ETA: 0s - loss: 0.5263 - accuracy: 0.7255\n",
      "Epoch 00130: val_loss did not improve from 0.51149\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.5262 - accuracy: 0.7233 - val_loss: 0.5290 - val_accuracy: 0.6800\n",
      "Epoch 131/200\n",
      "52/60 [=========================>....] - ETA: 0s - loss: 0.4888 - accuracy: 0.7519\n",
      "Epoch 00131: val_loss did not improve from 0.51149\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.4920 - accuracy: 0.7433 - val_loss: 0.5128 - val_accuracy: 0.6933\n",
      "Epoch 132/200\n",
      "50/60 [========================>.....] - ETA: 0s - loss: 0.4803 - accuracy: 0.7660\n",
      "Epoch 00132: val_loss improved from 0.51149 to 0.51119, saving model to temp.h5\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.4890 - accuracy: 0.7550 - val_loss: 0.5112 - val_accuracy: 0.6933\n",
      "Epoch 133/200\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.4982 - accuracy: 0.7271\n",
      "Epoch 00133: val_loss did not improve from 0.51119\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.4994 - accuracy: 0.7267 - val_loss: 0.5230 - val_accuracy: 0.6867\n",
      "Epoch 134/200\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.4908 - accuracy: 0.7386\n",
      "Epoch 00134: val_loss did not improve from 0.51119\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 0.4862 - accuracy: 0.7400 - val_loss: 0.5288 - val_accuracy: 0.6800\n",
      "Epoch 135/200\n",
      "55/60 [==========================>...] - ETA: 0s - loss: 0.5145 - accuracy: 0.7145\n",
      "Epoch 00135: val_loss did not improve from 0.51119\n",
      "60/60 [==============================] - 1s 8ms/step - loss: 0.5115 - accuracy: 0.7167 - val_loss: 0.5247 - val_accuracy: 0.6800\n",
      "Epoch 136/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.4715 - accuracy: 0.7533\n",
      "Epoch 00136: val_loss did not improve from 0.51119\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.4715 - accuracy: 0.7533 - val_loss: 0.5127 - val_accuracy: 0.7000\n",
      "Epoch 137/200\n",
      "52/60 [=========================>....] - ETA: 0s - loss: 0.4909 - accuracy: 0.7442\n",
      "Epoch 00137: val_loss did not improve from 0.51119\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.4877 - accuracy: 0.7417 - val_loss: 0.5203 - val_accuracy: 0.6867\n",
      "Epoch 138/200\n",
      "48/60 [=======================>......] - ETA: 0s - loss: 0.4613 - accuracy: 0.7708\n",
      "Epoch 00138: val_loss did not improve from 0.51119\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.4722 - accuracy: 0.7567 - val_loss: 0.5191 - val_accuracy: 0.6867\n",
      "Epoch 139/200\n",
      "55/60 [==========================>...] - ETA: 0s - loss: 0.4797 - accuracy: 0.7418\n",
      "Epoch 00139: val_loss did not improve from 0.51119\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.4865 - accuracy: 0.7300 - val_loss: 0.5263 - val_accuracy: 0.6800\n",
      "Epoch 140/200\n",
      "55/60 [==========================>...] - ETA: 0s - loss: 0.4970 - accuracy: 0.7491\n",
      "Epoch 00140: val_loss did not improve from 0.51119\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.5051 - accuracy: 0.7450 - val_loss: 0.5357 - val_accuracy: 0.6800\n",
      "Epoch 141/200\n",
      "55/60 [==========================>...] - ETA: 0s - loss: 0.4984 - accuracy: 0.7273\n",
      "Epoch 00141: val_loss did not improve from 0.51119\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.5008 - accuracy: 0.7233 - val_loss: 0.5305 - val_accuracy: 0.6867\n",
      "Epoch 142/200\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 0.4847 - accuracy: 0.7500\n",
      "Epoch 00142: val_loss did not improve from 0.51119\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.4870 - accuracy: 0.7467 - val_loss: 0.5198 - val_accuracy: 0.6733\n",
      "Epoch 143/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.4692 - accuracy: 0.7633\n",
      "Epoch 00143: val_loss did not improve from 0.51119\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 0.4692 - accuracy: 0.7633 - val_loss: 0.5167 - val_accuracy: 0.6667\n",
      "Epoch 144/200\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.5046 - accuracy: 0.7322\n",
      "Epoch 00144: val_loss did not improve from 0.51119\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.5031 - accuracy: 0.7333 - val_loss: 0.5183 - val_accuracy: 0.6733\n",
      "Epoch 145/200\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.4736 - accuracy: 0.7333\n",
      "Epoch 00145: val_loss did not improve from 0.51119\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.4736 - accuracy: 0.7300 - val_loss: 0.5133 - val_accuracy: 0.6733\n",
      "Epoch 146/200\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.4586 - accuracy: 0.7561\n",
      "Epoch 00146: val_loss did not improve from 0.51119\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.4760 - accuracy: 0.7600 - val_loss: 0.5201 - val_accuracy: 0.6867\n",
      "Epoch 147/200\n",
      "53/60 [=========================>....] - ETA: 0s - loss: 0.4848 - accuracy: 0.7491\n",
      "Epoch 00147: val_loss did not improve from 0.51119\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.4938 - accuracy: 0.7367 - val_loss: 0.5307 - val_accuracy: 0.6867\n",
      "Epoch 148/200\n",
      "54/60 [==========================>...] - ETA: 0s - loss: 0.4839 - accuracy: 0.7407\n",
      "Epoch 00148: val_loss did not improve from 0.51119\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.4806 - accuracy: 0.7417 - val_loss: 0.5156 - val_accuracy: 0.6867\n",
      "Epoch 149/200\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.4687 - accuracy: 0.7614\n",
      "Epoch 00149: val_loss did not improve from 0.51119\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.4645 - accuracy: 0.7650 - val_loss: 0.5141 - val_accuracy: 0.7000\n",
      "Epoch 150/200\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 0.4669 - accuracy: 0.7661\n",
      "Epoch 00150: val_loss did not improve from 0.51119\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.4675 - accuracy: 0.7667 - val_loss: 0.5187 - val_accuracy: 0.6867\n",
      "Epoch 151/200\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.4816 - accuracy: 0.7614\n",
      "Epoch 00151: val_loss did not improve from 0.51119\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.4836 - accuracy: 0.7600 - val_loss: 0.5207 - val_accuracy: 0.6733\n",
      "Epoch 152/200\n",
      "53/60 [=========================>....] - ETA: 0s - loss: 0.5118 - accuracy: 0.7396\n",
      "Epoch 00152: val_loss did not improve from 0.51119\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.5013 - accuracy: 0.7433 - val_loss: 0.5232 - val_accuracy: 0.6800\n",
      "Epoch 153/200\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.4899 - accuracy: 0.7561\n",
      "Epoch 00153: val_loss did not improve from 0.51119\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.4929 - accuracy: 0.7517 - val_loss: 0.5190 - val_accuracy: 0.6800\n",
      "Epoch 154/200\n",
      "49/60 [=======================>......] - ETA: 0s - loss: 0.5105 - accuracy: 0.7347\n",
      "Epoch 00154: val_loss did not improve from 0.51119\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.5043 - accuracy: 0.7367 - val_loss: 0.5140 - val_accuracy: 0.7000\n",
      "Epoch 155/200\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.4887 - accuracy: 0.7441\n",
      "Epoch 00155: val_loss did not improve from 0.51119\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.4849 - accuracy: 0.7467 - val_loss: 0.5130 - val_accuracy: 0.6933\n",
      "Epoch 156/200\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.4732 - accuracy: 0.7356\n",
      "Epoch 00156: val_loss did not improve from 0.51119\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.4728 - accuracy: 0.7350 - val_loss: 0.5154 - val_accuracy: 0.6933\n",
      "Epoch 157/200\n",
      "44/60 [=====================>........] - ETA: 0s - loss: 0.4827 - accuracy: 0.7386\n",
      "Epoch 00157: val_loss did not improve from 0.51119\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 0.4776 - accuracy: 0.7450 - val_loss: 0.5175 - val_accuracy: 0.6867\n",
      "Epoch 158/200\n",
      "48/60 [=======================>......] - ETA: 0s - loss: 0.4753 - accuracy: 0.7583\n",
      "Epoch 00158: val_loss did not improve from 0.51119\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.4760 - accuracy: 0.7583 - val_loss: 0.5127 - val_accuracy: 0.6800\n",
      "Epoch 159/200\n",
      "47/60 [======================>.......] - ETA: 0s - loss: 0.4846 - accuracy: 0.7383\n",
      "Epoch 00159: val_loss did not improve from 0.51119\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 0.4971 - accuracy: 0.7267 - val_loss: 0.5177 - val_accuracy: 0.6800\n",
      "Epoch 160/200\n",
      "53/60 [=========================>....] - ETA: 0s - loss: 0.4700 - accuracy: 0.7585\n",
      "Epoch 00160: val_loss did not improve from 0.51119\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 0.4829 - accuracy: 0.7467 - val_loss: 0.5127 - val_accuracy: 0.6800\n",
      "Epoch 161/200\n",
      "48/60 [=======================>......] - ETA: 0s - loss: 0.4759 - accuracy: 0.7458\n",
      "Epoch 00161: val_loss did not improve from 0.51119\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.4756 - accuracy: 0.7500 - val_loss: 0.5123 - val_accuracy: 0.6800\n",
      "Epoch 162/200\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.4959 - accuracy: 0.7379\n",
      "Epoch 00162: val_loss did not improve from 0.51119\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 0.4935 - accuracy: 0.7383 - val_loss: 0.5163 - val_accuracy: 0.6800\n",
      "Epoch 163/200\n",
      "54/60 [==========================>...] - ETA: 0s - loss: 0.4542 - accuracy: 0.7630\n",
      "Epoch 00163: val_loss improved from 0.51119 to 0.50444, saving model to temp.h5\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 0.4635 - accuracy: 0.7583 - val_loss: 0.5044 - val_accuracy: 0.6933\n",
      "Epoch 164/200\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.4694 - accuracy: 0.7458\n",
      "Epoch 00164: val_loss improved from 0.50444 to 0.49583, saving model to temp.h5\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.4687 - accuracy: 0.7450 - val_loss: 0.4958 - val_accuracy: 0.6800\n",
      "Epoch 165/200\n",
      "51/60 [========================>.....] - ETA: 0s - loss: 0.4599 - accuracy: 0.7667\n",
      "Epoch 00165: val_loss did not improve from 0.49583\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.4740 - accuracy: 0.7533 - val_loss: 0.4992 - val_accuracy: 0.6867\n",
      "Epoch 166/200\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.4635 - accuracy: 0.7702\n",
      "Epoch 00166: val_loss did not improve from 0.49583\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.4754 - accuracy: 0.7567 - val_loss: 0.4998 - val_accuracy: 0.6933\n",
      "Epoch 167/200\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 0.4768 - accuracy: 0.7464\n",
      "Epoch 00167: val_loss did not improve from 0.49583\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.4773 - accuracy: 0.7500 - val_loss: 0.5083 - val_accuracy: 0.6933\n",
      "Epoch 168/200\n",
      "55/60 [==========================>...] - ETA: 0s - loss: 0.4893 - accuracy: 0.7436\n",
      "Epoch 00168: val_loss did not improve from 0.49583\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.4872 - accuracy: 0.7500 - val_loss: 0.5030 - val_accuracy: 0.6867\n",
      "Epoch 169/200\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.4979 - accuracy: 0.7586\n",
      "Epoch 00169: val_loss did not improve from 0.49583\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.4948 - accuracy: 0.7600 - val_loss: 0.5115 - val_accuracy: 0.6933\n",
      "Epoch 170/200\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.4784 - accuracy: 0.7414\n",
      "Epoch 00170: val_loss did not improve from 0.49583\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.4760 - accuracy: 0.7433 - val_loss: 0.5120 - val_accuracy: 0.6933\n",
      "Epoch 171/200\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 0.4906 - accuracy: 0.7339\n",
      "Epoch 00171: val_loss did not improve from 0.49583\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.4886 - accuracy: 0.7333 - val_loss: 0.5222 - val_accuracy: 0.6800\n",
      "Epoch 172/200\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.4671 - accuracy: 0.7362\n",
      "Epoch 00172: val_loss did not improve from 0.49583\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.4740 - accuracy: 0.7367 - val_loss: 0.5084 - val_accuracy: 0.6933\n",
      "Epoch 173/200\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.4913 - accuracy: 0.7414\n",
      "Epoch 00173: val_loss did not improve from 0.49583\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.4895 - accuracy: 0.7450 - val_loss: 0.5045 - val_accuracy: 0.6933\n",
      "Epoch 174/200\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.4783 - accuracy: 0.7614\n",
      "Epoch 00174: val_loss did not improve from 0.49583\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.4736 - accuracy: 0.7667 - val_loss: 0.5168 - val_accuracy: 0.6867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 175/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.4679 - accuracy: 0.7550\n",
      "Epoch 00175: val_loss did not improve from 0.49583\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.4679 - accuracy: 0.7550 - val_loss: 0.5169 - val_accuracy: 0.7000\n",
      "Epoch 176/200\n",
      "52/60 [=========================>....] - ETA: 0s - loss: 0.4774 - accuracy: 0.7481\n",
      "Epoch 00176: val_loss did not improve from 0.49583\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.4754 - accuracy: 0.7500 - val_loss: 0.5061 - val_accuracy: 0.6867\n",
      "Epoch 177/200\n",
      "54/60 [==========================>...] - ETA: 0s - loss: 0.4584 - accuracy: 0.7796\n",
      "Epoch 00177: val_loss did not improve from 0.49583\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.4632 - accuracy: 0.7700 - val_loss: 0.5047 - val_accuracy: 0.6933\n",
      "Epoch 178/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.4840 - accuracy: 0.7550\n",
      "Epoch 00178: val_loss did not improve from 0.49583\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 0.4840 - accuracy: 0.7550 - val_loss: 0.5106 - val_accuracy: 0.7000\n",
      "Epoch 179/200\n",
      "54/60 [==========================>...] - ETA: 0s - loss: 0.4749 - accuracy: 0.7574\n",
      "Epoch 00179: val_loss did not improve from 0.49583\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.4699 - accuracy: 0.7633 - val_loss: 0.5123 - val_accuracy: 0.7000\n",
      "Epoch 180/200\n",
      "51/60 [========================>.....] - ETA: 0s - loss: 0.4524 - accuracy: 0.7725\n",
      "Epoch 00180: val_loss did not improve from 0.49583\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.4603 - accuracy: 0.7683 - val_loss: 0.5129 - val_accuracy: 0.7000\n",
      "Epoch 181/200\n",
      "52/60 [=========================>....] - ETA: 0s - loss: 0.4662 - accuracy: 0.7558\n",
      "Epoch 00181: val_loss did not improve from 0.49583\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 0.4552 - accuracy: 0.7650 - val_loss: 0.5073 - val_accuracy: 0.6933\n",
      "Epoch 182/200\n",
      "59/60 [============================>.] - ETA: 0s - loss: 0.4793 - accuracy: 0.7441\n",
      "Epoch 00182: val_loss did not improve from 0.49583\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.4789 - accuracy: 0.7450 - val_loss: 0.5143 - val_accuracy: 0.6933\n",
      "Epoch 183/200\n",
      "54/60 [==========================>...] - ETA: 0s - loss: 0.4922 - accuracy: 0.7370\n",
      "Epoch 00183: val_loss did not improve from 0.49583\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.4871 - accuracy: 0.7450 - val_loss: 0.5175 - val_accuracy: 0.6867\n",
      "Epoch 184/200\n",
      "55/60 [==========================>...] - ETA: 0s - loss: 0.5038 - accuracy: 0.7382\n",
      "Epoch 00184: val_loss did not improve from 0.49583\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.5009 - accuracy: 0.7417 - val_loss: 0.5147 - val_accuracy: 0.7000\n",
      "Epoch 185/200\n",
      "52/60 [=========================>....] - ETA: 0s - loss: 0.4504 - accuracy: 0.7731\n",
      "Epoch 00185: val_loss did not improve from 0.49583\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.4618 - accuracy: 0.7650 - val_loss: 0.5078 - val_accuracy: 0.6800\n",
      "Epoch 186/200\n",
      "52/60 [=========================>....] - ETA: 0s - loss: 0.4809 - accuracy: 0.7481\n",
      "Epoch 00186: val_loss did not improve from 0.49583\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.4866 - accuracy: 0.7400 - val_loss: 0.5113 - val_accuracy: 0.6933\n",
      "Epoch 187/200\n",
      "52/60 [=========================>....] - ETA: 0s - loss: 0.4908 - accuracy: 0.7519\n",
      "Epoch 00187: val_loss did not improve from 0.49583\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 0.4895 - accuracy: 0.7533 - val_loss: 0.5141 - val_accuracy: 0.6867\n",
      "Epoch 188/200\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 0.4872 - accuracy: 0.7357\n",
      "Epoch 00188: val_loss did not improve from 0.49583\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.4858 - accuracy: 0.7400 - val_loss: 0.5156 - val_accuracy: 0.6867\n",
      "Epoch 189/200\n",
      "54/60 [==========================>...] - ETA: 0s - loss: 0.4719 - accuracy: 0.7500\n",
      "Epoch 00189: val_loss did not improve from 0.49583\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.4626 - accuracy: 0.7617 - val_loss: 0.5136 - val_accuracy: 0.6933\n",
      "Epoch 190/200\n",
      "57/60 [===========================>..] - ETA: 0s - loss: 0.4612 - accuracy: 0.7579\n",
      "Epoch 00190: val_loss did not improve from 0.49583\n",
      "60/60 [==============================] - 0s 6ms/step - loss: 0.4608 - accuracy: 0.7583 - val_loss: 0.5161 - val_accuracy: 0.6933\n",
      "Epoch 191/200\n",
      "53/60 [=========================>....] - ETA: 0s - loss: 0.4623 - accuracy: 0.7509\n",
      "Epoch 00191: val_loss did not improve from 0.49583\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.4661 - accuracy: 0.7533 - val_loss: 0.5123 - val_accuracy: 0.6800\n",
      "Epoch 192/200\n",
      "60/60 [==============================] - ETA: 0s - loss: 0.4697 - accuracy: 0.7450\n",
      "Epoch 00192: val_loss did not improve from 0.49583\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.4697 - accuracy: 0.7450 - val_loss: 0.5137 - val_accuracy: 0.6933\n",
      "Epoch 193/200\n",
      "58/60 [============================>.] - ETA: 0s - loss: 0.4600 - accuracy: 0.7638\n",
      "Epoch 00193: val_loss did not improve from 0.49583\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.4568 - accuracy: 0.7683 - val_loss: 0.5079 - val_accuracy: 0.6800\n",
      "Epoch 194/200\n",
      "52/60 [=========================>....] - ETA: 0s - loss: 0.4799 - accuracy: 0.7519\n",
      "Epoch 00194: val_loss did not improve from 0.49583\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.4708 - accuracy: 0.7617 - val_loss: 0.5083 - val_accuracy: 0.6800\n",
      "Epoch 195/200\n",
      "56/60 [===========================>..] - ETA: 0s - loss: 0.4907 - accuracy: 0.7429\n",
      "Epoch 00195: val_loss did not improve from 0.49583\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.4863 - accuracy: 0.7467 - val_loss: 0.5038 - val_accuracy: 0.6933\n",
      "Epoch 196/200\n",
      "55/60 [==========================>...] - ETA: 0s - loss: 0.4458 - accuracy: 0.7709\n",
      "Epoch 00196: val_loss did not improve from 0.49583\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.4475 - accuracy: 0.7667 - val_loss: 0.5051 - val_accuracy: 0.7000\n",
      "Epoch 197/200\n",
      "54/60 [==========================>...] - ETA: 0s - loss: 0.4518 - accuracy: 0.7556\n",
      "Epoch 00197: val_loss did not improve from 0.49583\n",
      "60/60 [==============================] - 0s 8ms/step - loss: 0.4561 - accuracy: 0.7517 - val_loss: 0.5172 - val_accuracy: 0.6933\n",
      "Epoch 198/200\n",
      "53/60 [=========================>....] - ETA: 0s - loss: 0.4597 - accuracy: 0.7585\n",
      "Epoch 00198: val_loss did not improve from 0.49583\n",
      "60/60 [==============================] - 1s 9ms/step - loss: 0.4621 - accuracy: 0.7583 - val_loss: 0.5115 - val_accuracy: 0.7067\n",
      "Epoch 199/200\n",
      "50/60 [========================>.....] - ETA: 0s - loss: 0.4745 - accuracy: 0.7420\n",
      "Epoch 00199: val_loss did not improve from 0.49583\n",
      "60/60 [==============================] - 0s 5ms/step - loss: 0.4786 - accuracy: 0.7483 - val_loss: 0.5018 - val_accuracy: 0.7133\n",
      "Epoch 200/200\n",
      "50/60 [========================>.....] - ETA: 0s - loss: 0.4760 - accuracy: 0.7300\n",
      "Epoch 00200: val_loss did not improve from 0.49583\n",
      "60/60 [==============================] - 0s 7ms/step - loss: 0.4723 - accuracy: 0.7417 - val_loss: 0.5088 - val_accuracy: 0.7067\n"
     ]
    }
   ],
   "source": [
    "# fit the keras model on the dataset\n",
    "\n",
    "history = model.fit(X_train, y_train, validation_split=0.2, epochs=200, batch_size=10, verbose=1, callbacks=keras_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD4CAYAAAAXUaZHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAO/klEQVR4nO3df6zdd13H8efLlYkbwjp6V8vG6NAiDLLJvE5+GKiZhF9qh7JYFGnmYqNBRGOUQSIrISQzGoOCQJqBlEhGFn64ioAsF8eihI2OlbGtw1UWS6GuF1AQNEC3t3+c79jlckvPPd9zzj377PlImnO+v+73le/tfd3P+X7P99xUFZKktvzQWgeQJI2f5S5JDbLcJalBlrskNchyl6QGrVvrAAAbNmyozZs3r3UMSXpQufnmm79cVXMrLZuJct+8eTP79u1b6xiS9KCS5D+Ot8zTMpLUIMtdkhpkuUtSgyx3SWqQ5S5JDbLcJalBlrskNchyl6QGWe6S1KCZuENVktbcrbvWZr/nTWa/jtwlqUGWuyQ1yHKXpAZZ7pLUIMtdkhpkuUtSgyx3SWqQ5S5JDbLcJalBlrskNchyl6QGWe6S1CDLXZIaZLlLUoMsd0lqkOUuSQ06YbkneUeSo0luWzLv9CTXJbmre1y/ZNmrkxxM8rkkz51UcEnS8Q0zcn8n8Lxl8y4HFqpqC7DQTZPkXGA78ORum7ckOWlsaSVJQzlhuVfVDcBXl83eBuzpnu8BLl4y/z1V9a2quhs4CFw4pqySpCGNes59Y1UdAegez+jmnwl8Ycl6h7t53yfJziT7kuxbXFwcMYYkaSXjvqCaFebVSitW1e6qmq+q+bm5uTHHkKSHtlHL/Z4kmwC6x6Pd/MPAY5esdxbwpdHjSZJGMWq57wV2dM93ANcumb89yQ8nOQfYAtzUL6IkabXWnWiFJFcDW4ENSQ4DVwBXAtckuQw4BFwCUFW3J7kGuAM4Bry8qu6dUHZJ0nGcsNyr6iXHWXTRcdZ/A/CGPqEkSf14h6okNchyl6QGWe6S1CDLXZIaZLlLUoMsd0lqkOUuSQ2y3CWpQZa7JDXIcpekBlnuktQgy12SGmS5S1KDLHdJapDlLkkNstwlqUGWuyQ1yHKXpAZZ7pLUIMtdkhpkuUtSgyx3SWqQ5S5JDbLcJalBlrskNchyl6QGWe6S1CDLXZIa1Kvck/xhktuT3Jbk6iQPT3J6kuuS3NU9rh9XWEnScEYu9yRnAr8PzFfVU4CTgO3A5cBCVW0BFrppSdIU9T0tsw74kSTrgFOALwHbgD3d8j3AxT33IUlapZHLvaq+CPwFcAg4Anytqj4KbKyqI906R4AzxhFUkjS8Pqdl1jMYpZ8DPAY4NclLV7H9ziT7kuxbXFwcNYYkaQV9Tsv8AnB3VS1W1XeA9wPPAO5Jsgmgezy60sZVtbuq5qtqfm5urkcMSdJyfcr9EPC0JKckCXARcADYC+zo1tkBXNsvoiRptdaNumFV3ZjkvcCngWPALcBu4BHANUkuY/AL4JJxBJUkDW/kcgeoqiuAK5bN/haDUbwkaY14h6okNchyl6QGWe6S1CDLXZIaZLlLUoMsd0lqkOUuSQ2y3CWpQZa7JDXIcpekBlnuktQgy12SGmS5S1KDLHdJapDlLkkNstwlqUGWuyQ1yHKXpAZZ7pLUIMtdkhpkuUtSgyx3SWqQ5S5JDbLcJalBlrskNchyl6QGWe6S1CDLXZIaZLlLUoN6lXuS05K8N8mdSQ4keXqS05Ncl+Su7nH9uMJKkobTd+T+V8BHquqJwPnAAeByYKGqtgAL3bQkaYpGLvckjwSeBbwdoKq+XVX/DWwD9nSr7QEu7htSkrQ6fUbujwcWgb9NckuSq5KcCmysqiMA3eMZK22cZGeSfUn2LS4u9oghSVquT7mvAy4A3lpVTwW+ySpOwVTV7qqar6r5ubm5HjEkScv1KffDwOGqurGbfi+Dsr8nySaA7vFov4iSpNUaudyr6j+BLyT5yW7WRcAdwF5gRzdvB3Btr4SSpFVb13P7VwDvTnIy8HngUga/MK5JchlwCLik5z4kSavUq9yraj8wv8Kii/p8XUlSP96hKkkNstwlqUGWuyQ1yHKXpAZZ7pLUIMtdkhpkuUtSgyx3SWqQ5S5JDer78QMCuHXX2u37vDXct6SZ5chdkhpkuUtSgyx3SWqQ5S5JDbLcJalBlrskNchyl6QGWe6S1CDLXZIaZLlLUoMsd0lqkOUuSQ2y3CWpQZa7JDXIcpekBlnuktQgy12SGmS5S1KDepd7kpOS3JLkg9306UmuS3JX97i+f0xJ0mqMY+T+SuDAkunLgYWq2gIsdNOSpCnqVe5JzgJeCFy1ZPY2YE/3fA9wcZ99SJJWr+/I/Y3AnwD3LZm3saqOAHSPZ6y0YZKdSfYl2be4uNgzhiRpqZHLPckvAker6uZRtq+q3VU1X1Xzc3Nzo8aQJK1gXY9tnwn8cpIXAA8HHpnk74B7kmyqqiNJNgFHxxFUkjS8kUfuVfXqqjqrqjYD24GPVdVLgb3Ajm61HcC1vVNKklZlEu9zvxJ4TpK7gOd005KkKepzWua7qup64Pru+VeAi8bxdSVJo/EOVUlqkOUuSQ2y3CWpQZa7JDXIcpekBlnuktQgy12SGmS5S1KDLHdJapDlLkkNstwlqUGWuyQ1yHKXpAZZ7pLUIMtdkhpkuUtSgyx3SWqQ5S5JDRrLn9kTXP/xfttvffZ4ckgSOHKXpCZZ7pLUIMtdkhrkOXdg165+22999FhiSNLYOHKXpAZZ7pLUIMtdkhpkuUtSgyx3SWrQyOWe5LFJ/jnJgSS3J3llN//0JNcluat7XD++uJKkYfQZuR8D/qiqngQ8DXh5knOBy4GFqtoCLHTTkqQpGrncq+pIVX26e/4/wAHgTGAbsKdbbQ9wcd+QkqTVGctNTEk2A08FbgQ2VtURGPwCSHLGcbbZCewEOPvss8cR46Hp1l1rs9/z1mi/kobS+4JqkkcA7wP+oKq+Pux2VbW7quaran5ubq5vDEnSEr3KPcnDGBT7u6vq/d3se5Js6pZvAo72iyhJWq0+75YJ8HbgQFX95ZJFe4Ed3fMdwLWjx5MkjaLPOfdnAr8JfDbJ/m7ea4ArgWuSXAYcAi7pF1GStFojl3tV/QuQ4yy+aNSvK0nqzztUJalBlrskNchyl6QGWe6S1CDLXZIaZLlLUoMsd0lqkOUuSQ2y3CWpQZa7JDXIcpekBo3lj3WstV271jpBf9d/vP/X2Prs/l9DUhscuUtSgyx3SWqQ5S5JDWrinPv9tj5611pHeOjwD3NLM82RuyQ1yHKXpAZZ7pLUoKbOuT/U9X2vvO+Tl9rhyF2SGuTIXdJsWat3YjXGkbskNchyl6QGeVpGDy4PtZfs3rSlETlyl6QGOXJXc5p6S+hD7ZWKxsaRuyQ1yJG7xqqFUbN/OEUtmNjIPcnzknwuycEkl09qP5Kk7zeRkXuSk4C/AZ4DHAY+lWRvVd0xif1pPMYxYm0hg9SCSY3cLwQOVtXnq+rbwHuAbRPalyRpmUmdcz8T+MKS6cPAzy5dIclOYGc3+Y0kn1uyeAPw5dXu9HWr3WB0I+WbklnOBrOdz2yjm+V8s5wNeF2ffI873oJJlXtWmFffM1G1G9i94sbJvqqan0SwcZjlfLOcDWY7n9lGN8v5ZjkbTC7fpE7LHAYeu2T6LOBLE9qXJGmZSZX7p4AtSc5JcjKwHdg7oX1JkpaZyGmZqjqW5PeAfwJOAt5RVbev4kuseLpmhsxyvlnOBrOdz2yjm+V8s5wNJpQvVXXitSRJDyp+/IAkNchyl6QGrWm5n+gjCpL8RpJbu3+fSHL+DGXb1uXan2Rfkp+bVrZh8i1Z72eS3JvkxbOSLcnWJF/rjt3+JK+dVrZh8i3JuD/J7Ummdt/sEMfuj5cct9u67+3pM5TvUUn+IclnumN36QxlW5/kA93P7U1JnjLFbO9IcjTJbcdZniR/3WW/NckFvXdaVWvyj8GF1n8HHg+cDHwGOHfZOs8A1nfPnw/cOEPZHsED1yzOA+6cpWO3ZL2PAR8CXjwr2YCtwAdn+P/dacAdwNnd9Bmzkm3Z+r8EfGzGjt1rgD/rns8BXwVOnpFsfw5c0T1/IrAwxWP3LOAC4LbjLH8B8GEG9wg9bRxdt5Yj9xN+REFVfaKq/qub/CSD98vPSrZvVPddAU5l2U1aa52v8wrgfcDRGcy2VobJ9+vA+6vqEEBVTev4rfbYvQS4eirJBobJV8CPJgmDAdBXgWMzku1cYAGgqu4ENifZOIVsVNUNDI7F8WwD3lUDnwROS7Kpzz7XstxX+oiCM3/A+pcx+M02DUNlS/KiJHcC/wj81pSywRD5kpwJvAh42xRzwfDf16d3L90/nOTJ04kGDJfvCcD6JNcnuTnJy2YoGwBJTgGex+CX97QMk+/NwJMY3LT4WeCVVXXfjGT7DPArAEkuZHDr/rQGjCey2j48obUs9xN+RMF3V0x+nkG5v2qiiZbscoV535etqj5QVU8ELgZeP/FUDxgm3xuBV1XVvVPIs9Qw2T4NPK6qzgfeBPz9xFM9YJh864CfBl4IPBf40yRPmHQwVvEzweCUzL9W1Q8aDY7bMPmeC+wHHgP8FPDmJI+cdDCGy3Ylg1/a+xm8qr2F6byqGMZqvvdDWcs/1jHURxQkOQ+4Cnh+VX1llrLdr6puSPLjSTZU1TQ+oGiYfPPAewavjtkAvCDJsaqadJGeMFtVfX3J8w8lecuMHbvDwJer6pvAN5PcAJwP/NsMZLvfdqZ7SgaGy3cpcGV3yvJgkrsZnN++aa2zdf/vLoXBBUzg7u7fLBj/R7ZM64LCChcQ1gGfB87hgQsgT162ztnAQeAZM5jtJ3jgguoFwBfvn56FfMvWfyfTu6A6zLH7sSXH7kLg0CwdOwanFRa6dU8BbgOeMgvZuvUexeD87anTOGarPHZvBXZ1zzd2PxcbZiTbaXQXd4HfZnCOe5rHbzPHv6D6Qr73gupNffe3ZiP3Os5HFCT5nW7524DXAo8G3tKNQI/VFD7dbchsvwq8LMl3gP8Dfq2679KM5FsTQ2Z7MfC7SY4xOHbbZ+nYVdWBJB8BbgXuA66qqhXfwjbtbN2qLwI+WoNXFlMzZL7XA+9M8lkGRfWqmsIrsiGzPQl4V5J7Gbwb6rJJ57pfkqsZvEtsQ5LDwBXAw5Zk+xCDd8wcBP6X7hVGr31O6WdKkjRF3qEqSQ2y3CWpQZa7JDXIcpekBlnuktQgy12SGmS5S1KD/h8kThdFcIf/mwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ind0 = np.where(y_test==0)\n",
    "ind1 = np.where(y_test==1)\n",
    "\n",
    "y_pred_0 = y_pred_[ind0]\n",
    "y_pred_1 = y_pred_[ind1]\n",
    "y_test_0 = y_test[ind0] \n",
    "y_test_1 = y_test[ind1]\n",
    "\n",
    "pyplot.figure()\n",
    "pyplot.scatter(y_pred_0 ,y_test_0)\n",
    "pyplot.scatter(y_pred_1, y_test_1)\n",
    "\n",
    "pyplot.figure()\n",
    "pyplot.hist(y_pred_0, 10, facecolor='blue', alpha=0.5)\n",
    "pyplot.hist(y_pred_1, 10, facecolor='orange', alpha=0.5)\n",
    "pyplot.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 82   8]\n",
      " [ 85 136]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.91      0.64        90\n",
      "           1       0.94      0.62      0.75       221\n",
      "\n",
      "    accuracy                           0.70       311\n",
      "   macro avg       0.72      0.76      0.69       311\n",
      "weighted avg       0.81      0.70      0.71       311\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.round(y_pred_)\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.where(y_pred == 1)[0]) #221"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(np.where(y_pred == 0)[0]) #90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.plot(history.history['loss'])\n",
    "pyplot.plot(history.history['val_loss'])\n",
    "pyplot.title('model loss')\n",
    "pyplot.ylabel('loss')\n",
    "pyplot.xlabel('epoch')\n",
    "pyplot.legend(['train', 'val'], loc='upper left')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 82   8]\n",
      " [ 85 136]]\n"
     ]
    }
   ],
   "source": [
    "confusion = metrics.confusion_matrix(y_test, y_pred)\n",
    "TP = confusion[1, 1]\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (TP + TN) / float(TP + TN + FP + FN)\n",
    "classification_error = (FP + FN) / float(TP + TN + FP + FN)\n",
    "sensitivity = TP / float(FN + TP)\n",
    "specificity = TN / (TN + FP)\n",
    "precision = TP / float(TP + FP)\n",
    "false_positive_rate = FP / float(TN + FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.7009646302250804\n",
      "Error rate :  0.2990353697749196\n",
      "Sensitvity :  0.6153846153846154\n",
      "Specificity :  0.9111111111111111\n",
      "Precision :  0.9444444444444444\n",
      "FPR :  0.08888888888888889\n"
     ]
    }
   ],
   "source": [
    "print ('Accuracy : ',accuracy)\n",
    "print ('Error rate : ',classification_error)\n",
    "print ('Sensitvity : ',sensitivity)\n",
    "print ('Specificity : ',specificity)\n",
    "print ('Precision : ',precision)\n",
    "print ('FPR : ',false_positive_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
